{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project Sputnik.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeiErfIfI1lx4YEDt/NvgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabrinser/Jupyter-notebooks/blob/master/Final_Project_Sputnik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiFTi69FjZyr"
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ii6qUMzjxZZ"
      },
      "source": [
        "sys.path.insert(0, rC:\\Users\\18479\\OneDrive\\chromedriver.exe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-eMp_4Bjx3D"
      },
      "source": [
        "import requests\n",
        "import selenium.webdriver as webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from bs4.element import Comment\n",
        "import urllib.request\n",
        "import time\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "#Import Textblob for sentiment analysis\n",
        "from textblob import TextBlob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spruahSSj4AL"
      },
      "source": [
        "import os\n",
        "\n",
        "#pull webpage and make a soup object\n",
        "page = requests.get('https://www.sputnik.com/')\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "#%% Get all sub-links on page\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h9fgtdHkkVw"
      },
      "source": [
        "articles = []\n",
        "hold_heads = []\n",
        "headlines = []\n",
        "hold_sub = []\n",
        "sub_articles = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCSjiYCikwkn"
      },
      "source": [
        "#scrape all URLs on main part of page\n",
        "for body in soup.find_all(class_=\"b-tabs_content\"): \n",
        "    for link in body.find_all(\"a\"):\n",
        "        articles.append(link.get('href'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCu06pOfkzdh"
      },
      "source": [
        "for story in articles: \n",
        "    if \"news\" in story:\n",
        "        hold_sub.append(story)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo0QGfcYk3-o"
      },
      "source": [
        "#remove duplicates by creating dictionary then \n",
        "#convert back to list; keeps order v set limitations\n",
        "sub_articles = list(OrderedDict.fromkeys(hold_sub)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FClI41LRk5mE"
      },
      "source": [
        "#grabbing all the headlines\n",
        "for title in soup.find_all(\"h1\"): \n",
        "    hold_heads.append(title.get_text().strip('\\n'))\n",
        "headlines = list(OrderedDict.fromkeys(hold_heads))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeT7CTyHcqe8"
      },
      "source": [
        "#add prefix to articles for use in later function using concatenation\n",
        "for i in range(len(sub_articles)): \n",
        "    if \"sputnik.com\" in sub_articles[i]:\n",
        "        continue\n",
        "    else:    \n",
        "        sub_articles[i] = 'https://www.sputnik.com' + sub_articles[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALVTZhl6cw2t"
      },
      "source": [
        "#%% headless browser and scrolling function\n",
        "def get_html_scroll(url):\n",
        "    \n",
        "    options = Options()\n",
        "    options.headless = True\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920x1080\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_driver = r'C:\\Users\\ryanp\\OneDrive\\Documents\\NIU\\MST 691 Data Science Tools and Techniques\\Code\\chromedriver.exe'\n",
        "    browser = webdriver.Chrome(chrome_driver, options = options)\n",
        "    browser.get(url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUHCBqPucz-e"
      },
      "source": [
        "    lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
        "    match=False\n",
        "    while(match==False):\n",
        "        lastCount = lenOfPage\n",
        "        time.sleep(4)\n",
        "        lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
        "        if lastCount==lenOfPage:\n",
        "          time.sleep(4)\n",
        "          match=True\n",
        "    post_elms = browser.page_source\n",
        "    return post_elms\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBtgWJ9c6fH"
      },
      "source": [
        "#%% Scrape sub-links content\n",
        "def build_dataset (sub_articles):\n",
        "    article_data = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ZnDluDc9RR"
      },
      "source": [
        "    for url in sub_articles:\n",
        "        data = get_html_scroll(url)        \n",
        "        soup = BeautifulSoup(data, 'html.parser')\n",
        "        text = []\n",
        "        for paragraph in soup.find_all('p'):\n",
        "            text.append(paragraph.get_text())\n",
        "        \n",
        "        news_articles = [{'article_headline': soup.find('h1').string,\n",
        "                          'article_contents': text}]\n",
        "        article_data.extend(news_articles)\n",
        "    df = pd.DataFrame(article_data)\n",
        "    df = df[['article_headline', 'article_contents']]\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ZBQin7c_b3"
      },
      "source": [
        "#%% run program\n",
        "news_df = build_dataset(sub_articles[:5])\n",
        "news_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}