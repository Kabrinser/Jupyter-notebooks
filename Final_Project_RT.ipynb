{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project RT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTBxDeRDsEUM/y6Uu0DfjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabrinser/Jupyter-notebooks/blob/master/Final_Project_RT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zzQuitjmhD5"
      },
      "source": [
        "import sys\n",
        "\n",
        "#path of wherever the chromedriver is located\n",
        "sys.path.insert(0, r'C:\\Users\\18479\\OneDrive\\chromedriver.exe')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRG0kngpW4c",
        "outputId": "eb03f641-e59f-4bee-dd02-ad6ae2f633fe"
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUtDCQuHmnve"
      },
      "source": [
        "import requests\n",
        "import selenium.webdriver as webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from bs4.element import Comment\n",
        "import urllib.request\n",
        "import time\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from textblob import TextBlob\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUNzBA4amrb9"
      },
      "source": [
        "import os"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpXa7zfxmyee"
      },
      "source": [
        "#pull webpage and make a soup object\n",
        "page = requests.get('https://www.rt.com/')\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "#%% Get all sub-links on page\n",
        "#empty lists for using below\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4ipO1fmm4Ni"
      },
      "source": [
        "articles = []\n",
        "hold_heads = []\n",
        "headlines = []\n",
        "hold_sub = []\n",
        "sub_articles = []\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6X4qEYNnFDH"
      },
      "source": [
        "#scrape all URLs on main part of page\n",
        "for body in soup.find_all(class_=\"columns_content\"): \n",
        "    for link in body.find_all(\"a\"):\n",
        "        articles.append(link.get('href'))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOZ0JkX8nJ4l"
      },
      "source": [
        "#pull out only the article URLs, leaving the advertisment\n",
        "#and other non-pertinent links\n",
        "for story in articles: \n",
        "    if \"news\" in story:\n",
        "        hold_sub.append(story)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsH_YcJSnMZ_"
      },
      "source": [
        "#remove duplicates by creating dictionary then \n",
        "#convert back to list; keeps order v set limitations\n",
        "sub_articles = list(OrderedDict.fromkeys(hold_sub)) \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsPO4dV5b1JC"
      },
      "source": [
        "#grabbing all the headlines\n",
        "for title in soup.find_all(\"h3\"): \n",
        "    hold_heads.append(title.get_text().strip('\\n'))\n",
        "headlines = list(OrderedDict.fromkeys(hold_heads))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZD45PYpnSiJ"
      },
      "source": [
        "#add prefix to articles for use in later function using concatenation\n",
        "for i in range(len(sub_articles)): \n",
        "    if \"rt.com\" in sub_articles[i]:\n",
        "        continue\n",
        "    else:    \n",
        "        sub_articles[i] = '#add prefix to articles for use in later function using concatenation'\n",
        "for i in range(len(sub_articles)): \n",
        "    if \"rt.com\" in sub_articles[i]:\n",
        "        continue\n",
        "    else:    \n",
        "        sub_articles[i] = 'https://www.rt.com/' + sub_articles[i]\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwDdaS7UnraJ"
      },
      "source": [
        "#%% headless browser and scrolling function\n",
        "def get_html_scroll(url):\n",
        "\n",
        "    options = Options()\n",
        "    options.headless = True\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920x1080\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_driver = r'C:\\Users\\18479\\OneDrive\\chromedriver.exe'\n",
        "    browser = webdriver.Chrome(chrome_driver, options = options)\n",
        "    browser.get(url)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "secyDyTEn0Fv",
        "outputId": "923f2aae-be5a-4e1b-e08f-480fad4d518e"
      },
      "source": [
        "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
        "match=False\n",
        "while(match==False):\n",
        "  lastCount = lenOfPage\n",
        "  time.sleep(4)\n",
        "  lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
        "  \n",
        "  if lastCount==lenOfPage:\n",
        "      time.sleep(4)\n",
        "      match=True\n",
        "  post_elms = browser.page_source\n",
        "  return post_elms\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f7cc2b813a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlenOfPage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlastCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenOfPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'browser' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q_qnbdmn2wh"
      },
      "source": [
        "#%% Scrape sub-links content\n",
        "def build_dataset (sub_articles):\n",
        "    article_data = []\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMNt4Uxqn435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "f03cc4c2-3bf7-41de-a9ed-6072bba2a2c5"
      },
      "source": [
        "for url in sub_articles:\n",
        "        data = get_html_scroll(url)        \n",
        "        soup = BeautifulSoup(data, 'html.parser')\n",
        "        text = []\n",
        "for paragraph in soup.find_all('p'):\n",
        "            text.append(paragraph.get_text())\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-777c1c5405df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "E3UsQO4Nn7UQ",
        "outputId": "4429a460-7164-4501-f173-57f8882dbb15"
      },
      "source": [
        "        news_articles = [{'article_headline': soup.find('h1').string,\n",
        "                          'article_contents': text}]\n",
        "        article_data.extend(news_articles)\n",
        "df = pd.DataFrame(article_data)\n",
        "df = df[['article_headline', 'article_contents']]\n",
        "  return df\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-9219d7b2a9be>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    return df\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "VWT7PbvNn9sp",
        "outputId": "4e6afd29-f447-40ab-f9ec-4d909884737e"
      },
      "source": [
        "#%% run program\n",
        "news_df = build_dataset(sub_articles[:5])\n",
        "news_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2832130b99c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%% run program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "sH-ghu-xRQ7m",
        "outputId": "343c264c-089a-4015-f962-c2fd80e8cf43"
      },
      "source": [
        "# I am not sure how we pull the text from the articles or usin the tags\n",
        "text = ''\n",
        "blob = TextBlob(text)\n",
        "blob.tags\n",
        "blob.noun_phrases\n",
        "\n",
        "for sentence in blob.sentences\n",
        "  print(sentence.sentiment.polarity)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-d06cb55d89f6>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for sentence in blob.sentences\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV8_8ZVQXKtx"
      },
      "source": [
        "#Useful to categorize sentences/paragraphs\n",
        "def getAnalysis(score):\n",
        "  if score < 0:\n",
        "    return 'Negative'\n",
        "  elif score == 0:\n",
        "    return 'Neutral'\n",
        "  else:\n",
        "    return 'Positive'\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}